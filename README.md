EmoReflector Project

EmoReflector is a social robot designed to interact with the elderly people based on emotions. It is implemented by a robust emotion recognition module using deep-learning models (CNN and ResNet-18)
and embeding the algorithm into a micro-controller, Raspberry Pi. The robot has a camera to capture facial videos, and the module classifies the emotions of the user into negative or non-negative emotions.
Based on the recognized emotions, if the emotions are classified negative, the robot will respond using an audio output.

Instructions on how to run the code: 
Step 1: Open the drive link in the GitHub.  
Step 2: Click on the executable file. 
Step 3: Google Colab will open. 
Step 4: Run the code. 

Additional Information: No need to re-train the model. Just skip that code and load the model 
as it is already saved in the same path as the executable file.  

If Raspberry Pi 4B with integrated camera and speaker module is available: 
Step 1: Open the Raspberry Pi executable text file in the Google Drive. 
Step 2: Copy the code in the text file and paste it in the IDE available in the Raspberry Pi. 
Step 3: Transfer the model.onnx available in the shared Google Drive link below into the same 
path of the code that was pasted in the previous step.  
Step 4: Add your email where the comment indicates. 
Step 5: Run the code. 
Step 6: Aim the camera towards a face. 

A link to the GitHub repository: 
https://github.com/Aafia1223/GP1_Group1_EmoReflector_Dr.Heyam.git 
A link to the Google Drive: 
https://drive.google.com/drive/folders/1JOOhLMOd4iXx2nrPqSp9wVXox46lwUcT?usp=sharing
